{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Reorderer\n",
    "\n",
    "This notebook provides a method to reorder the sentences in a text, using BERT For Next Sentence Prediction.\n",
    "It is an important NLP task, especially when you have some unordered text coming from different sources such as in Summarization o Multi-Summarization tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required dependencies and set-up\n",
    "\n",
    "The `transformers` library from HuggingFace to import BertTokenizer, BertForNextSentencePrediction is used. \n",
    "The tokenizer is used to tokenize the senteces used in the Next-Sentence-Prediction Model. The `bert-base-multilingual-cased` as pretrained weights are used to support cased text and several languages.\n",
    "\n",
    "The `tensorflow` library is used to build the tensors for the model.\n",
    "Finally, `tensorflow.keras` is used later to compute the softmax from the logits in the Next-Sentence-Prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:20:18.867251Z",
     "start_time": "2020-04-19T15:20:17.185809Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers nltk tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:17:31.237020Z",
     "start_time": "2020-04-19T15:17:31.233028Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForNextSentencePrediction\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:09:09.860756Z",
     "start_time": "2020-04-19T15:09:01.817538Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "nsp_model = TFBertForNextSentencePrediction.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "In this section, the function needed for the text reordering are explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_next_sentence_prob\n",
    "The `predict_next_sentence_prob` function aims to return the probability that the second sentence provided in input is the continuation of the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:21:23.328277Z",
     "start_time": "2020-04-19T15:21:23.322279Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_sentence_prob(sent1, sent2):\n",
    "    # encode the two sequences. Particularly, make clear that they must be \n",
    "    # encoded as \"one\" input to the model by using 'seq_B' as the 'text_pair'\n",
    "    # NOTE how the token_type_ids are 0 for all tokens in seq_A and 1 for seq_B, \n",
    "    # this way the model knows which token belongs to which sequence\n",
    "    encoded = tokenizer.encode_plus(sent1, text_pair=sent2)\n",
    "    encoded[\"input_ids\"] = tf.constant(encoded[\"input_ids\"])[None, :]\n",
    "    encoded[\"token_type_ids\"] = tf.constant(encoded[\"token_type_ids\"])[None, :]\n",
    "    encoded[\"attention_mask\"] = tf.constant(encoded[\"attention_mask\"])[None, :]\n",
    "\n",
    "    # a model's output is a tuple, we only need the output tensor containing\n",
    "    # the relationships which is the first item in the tuple\n",
    "    outputs = nsp_model(encoded)\n",
    "    seq_relationship_scores = outputs[0]\n",
    "\n",
    "    # we need softmax to convert the logits into probabilities\n",
    "    # index 0: sequence B is a continuation of sequence A\n",
    "    # index 1: sequence B is a random sequence\n",
    "    probs = tf.keras.activations.softmax(seq_relationship_scores, axis=-1)\n",
    "    return probs.numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following snippet the Next-Sentence-Prediction probabilities over two sentences are computed. You can see from the examples the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T16:02:28.353124Z",
     "start_time": "2020-04-19T16:02:27.362134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that sent2 is the next sentence with respect to sent1: 0.9987509\n",
      "Probability that sent1 is the next sentence with respect to sent2: 0.99957067\n",
      "Probability that a sentence is the next sentence with respect to itself: 0.008009296\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"So, the old man decided to go home to make his dog feel better.\",\n",
    "    \"In the park, there were also a mouse and the dog was frightened.\",\n",
    "    \"The dog felt better because the man gave 3 biscuits to him.\",\n",
    "    \"One day, an old man went to the park with his dog.\",\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Probability that sent2 is the next sentence with respect to sent1:\",\n",
    "      predict_next_sentence_prob(sentences[0], sentences[1]))\n",
    "print(\"Probability that sent1 is the next sentence with respect to sent2:\",\n",
    "      predict_next_sentence_prob(sentences[1], sentences[0])) # better\n",
    "print(\"Probability that a sentence is the next sentence with respect to itself:\",\n",
    "      predict_next_sentence_prob(sentences[1], sentences[1])) # very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_correlation_matrix\n",
    "The `create_correlation_matrix` takes a string array-like and create a correlation matrix which takes care of the all probabilities computed over the sentences given in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:59:21.672998Z",
     "start_time": "2020-04-19T15:59:21.664000Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_correlation_matrix(sentences: list):\n",
    "    num_sentences = len(sentences) \n",
    "    correlation_matrix = np.empty(shape=(num_sentences, num_sentences), dtype=float, order='C')\n",
    "    for i, s1 in enumerate(sentences):\n",
    "        for j, s2 in enumerate(sentences):\n",
    "            correlation_matrix[i][j] = predict_next_sentence_prob(s1, s2)\n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:58:07.697736Z",
     "start_time": "2020-04-19T15:58:01.988815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00227171, 0.99875093, 0.99981552, 0.99655944],\n",
       "       [0.99957067, 0.0080093 , 0.9995746 , 0.99964833],\n",
       "       [0.99780148, 0.99955863, 0.00385047, 0.99557221],\n",
       "       [0.99929106, 0.99986291, 0.99985898, 0.00212961]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = create_correlation_matrix(sentences)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reorder_sentences\n",
    "The `reorder_sentences` takes a string array-like. It creates a correlation matrix with the previous function and iteratively finds the best association in the matrix. It deletes the sentences selected in this way and continues until no association can be done. The association is expressed as the tuple (X, Y), where X is the first sentence and the Y is the most probable next sentence. The latter is used as the new first sentence from which search the best next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T15:58:07.711738Z",
     "start_time": "2020-04-19T15:58:07.705738Z"
    }
   },
   "outputs": [],
   "source": [
    "def reorder_sentences(sentences: list):\n",
    "    ordering = []\n",
    "    correlation_matrix = create_correlation_matrix(sentences)\n",
    "    hint = None\n",
    "    while correlation_matrix.any():\n",
    "        if hint == None:\n",
    "            ind = np.unravel_index(np.argmax(correlation_matrix, axis=None), correlation_matrix.shape)\n",
    "        else:\n",
    "            ind = np.unravel_index(np.argmax(correlation_matrix[hint,:], axis=None), correlation_matrix[hint,:].shape)\n",
    "            ind = (hint, ind[0])\n",
    "        hint = ind[1]    \n",
    "        correlation_matrix[ind[0], :] = 0\n",
    "        correlation_matrix[:, ind[0]] = 0\n",
    "        ordering.append(ind[0])\n",
    "    return ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T16:02:37.971021Z",
     "start_time": "2020-04-19T16:02:32.929043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 1, 0, 2],\n",
       " ['One day, an old man went to the park with his dog.',\n",
       "  'In the park, there were also a mouse and the dog was frightened.',\n",
       "  'So, the old man decided to go home to make his dog feel better.',\n",
       "  'The dog felt better because the man gave 3 biscuits to him.'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordering = reorder_sentences(sentences)\n",
    "reordered_sentences = [sentences[idx] for idx in ordering]\n",
    "ordering, reordered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "This example shows how the text-reodered works over a medium long non-ordered sentences. Correct order is [2, 0, 1, 3, 5, 4], but it found another that fits well [2, 0, 1, 5, 4, 3]. Let's read the text to get a proof.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T19:20:15.021178Z",
     "start_time": "2020-04-19T19:19:58.272944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 0, 1, 5, 4, 3],\n",
       " [\"napoleon bonaparte (ajaccio, 15 august 1769 - longwood, sant'elena island, 5 may 1821) was a french politician and general, founder of the first french empire and protagonist of the first phase of european contemporary history called napoleonic age .\",\n",
       "  \"famous thanks to the victories obtained during the first campaign of Italy, after the coup d'etat of the 18th brumaire (9 November 1799) he assumed power in france: he was first consul from November of that year to 18 May 1804, and emperor of the French, with the name of napoleon i (napoléon ier) from 2 December 1804 to 14 April 1814 and again from 20 March to 22 June 1815 .\",\n",
       "  'thanks to his system of alliances and a series of brilliant victories against European powers, he conquered and governed a large part of continental Europe, exporting the revolutionary ideals of social renewal and managing to control numerous kingdoms through people loyal to him (giuseppe bonaparte in spain , joachim murat in the kingdom of naples, girolamo bonaparte in westphalia, jean-baptiste jules bernadotte in the kingdom of sweden and luigi bonaparte in the kingdom of holland) .',\n",
       "  'napoleon was defeated in the battle of Leipzig by the European allies in October 1813 and he abdicated on April 4, 1814, and was exiled to the island of Elba : it marked the decline of his dominion over Europe .',\n",
       "  'in march 1815, stealthily abandoned the island, he landed in Golfe Juan, near Antibes and returned to Paris without encountering opposition, regaining power for the so-called << one hundred days >> period, until he was definitively defeated by the seventh coalition in the battle of Waterloo, 18 June 1815; he spent the last years of his life in exile on the island of Saint Helena, under the control of the British .',\n",
       "  \"great man of war, protagonist of over twenty years of campaigns in europe, napoleon was considered the greatest strategist in history by the military historian basil liddell hart, while the historian evgenij tàrle does not hesitate to define him `` the incomparable master of art of the war `` is `` the greatest of the great '' .\"])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences =  [\n",
    "    \"famous thanks to the victories obtained during the first campaign of Italy, after the coup d'etat of the 18th brumaire (9 November 1799) he assumed power in france: he was first consul from November of that year to 18 May 1804, and emperor of the French, with the name of napoleon i (napoléon ier) from 2 December 1804 to 14 April 1814 and again from 20 March to 22 June 1815 .\",\n",
    "    \"thanks to his system of alliances and a series of brilliant victories against European powers, he conquered and governed a large part of continental Europe, exporting the revolutionary ideals of social renewal and managing to control numerous kingdoms through people loyal to him (giuseppe bonaparte in spain , joachim murat in the kingdom of naples, girolamo bonaparte in westphalia, jean-baptiste jules bernadotte in the kingdom of sweden and luigi bonaparte in the kingdom of holland) .\",\n",
    "    \"napoleon bonaparte (ajaccio, 15 august 1769 - longwood, sant'elena island, 5 may 1821) was a french politician and general, founder of the first french empire and protagonist of the first phase of european contemporary history called napoleonic age .\",\n",
    "    \"great man of war, protagonist of over twenty years of campaigns in europe, napoleon was considered the greatest strategist in history by the military historian basil liddell hart, while the historian evgenij tàrle does not hesitate to define him `` the incomparable master of art of the war `` is `` the greatest of the great '' .\",\n",
    "    \"in march 1815, stealthily abandoned the island, he landed in Golfe Juan, near Antibes and returned to Paris without encountering opposition, regaining power for the so-called << one hundred days >> period, until he was definitively defeated by the seventh coalition in the battle of Waterloo, 18 June 1815; he spent the last years of his life in exile on the island of Saint Helena, under the control of the British .\",\n",
    "    \"napoleon was defeated in the battle of Leipzig by the European allies in October 1813 and he abdicated on April 4, 1814, and was exiled to the island of Elba : it marked the decline of his dominion over Europe .\",\n",
    "]\n",
    "ordering = reorder_sentences(sentences)\n",
    "reordered_sentences = [sentences[idx] for idx in ordering]\n",
    "ordering, reordered_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
